{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84af100-9192-4e22-be37-a9bfbc4b7f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching Google Trends data for 'Bitcoin'...\n",
      "Attempt 1 of 5 to fetch Google Trends data...\n",
      "Attempt 1 failed: The request failed: Google returned a response with code 429\n",
      "Retrying in 60 seconds...\n"
     ]
    }
   ],
   "source": [
    "# --- Final Code Modification: 1-Year Data, 1-Day Prediction, Tuned XGBoost (Google Trends Feature Added) ---\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "# === Step 1: Data Collection (Using yfinance, 1 Year) ===\n",
    "ticker = \"BTC-USD\"\n",
    "# print(f\"Starting data collection for {ticker} (Last 1 Year)...\") # Removed\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=365) # 1 Year\n",
    "start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "try:\n",
    "    btc_data = yf.download(ticker, start=start_date_str, end=end_date_str, progress=False)\n",
    "    if btc_data.empty: raise ValueError(f\"No data downloaded for {ticker}\")\n",
    "    # print(f\"Downloaded {len(btc_data)} rows for {ticker}\") # Removed\n",
    "\n",
    "    # --- Column Name Handling ---\n",
    "    # print(f\"Original columns type: {type(btc_data.columns)}\") # Removed\n",
    "    if isinstance(btc_data.columns, pd.MultiIndex):\n",
    "        # print(\"Detected MultiIndex columns. Flattening...\") # Removed\n",
    "        new_cols = [];\n",
    "        for col_tuple in btc_data.columns.values:\n",
    "            standard_name = None\n",
    "            for level in col_tuple:\n",
    "                if isinstance(level, str) and level.capitalize() in ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']:\n",
    "                    standard_name = level.capitalize(); break\n",
    "            if standard_name: new_cols.append(standard_name)\n",
    "            else: new_cols.append('_'.join(filter(None, map(str, col_tuple))).strip())\n",
    "        btc_data.columns = new_cols\n",
    "        if 'Adj Close' in btc_data.columns and 'Close' in btc_data.columns: btc_data.drop(columns=['Adj Close'], inplace=True, errors='ignore')\n",
    "        elif 'Adj Close' in btc_data.columns and 'Close' not in btc_data.columns: btc_data.rename(columns={'Adj Close': 'Close'}, inplace=True)\n",
    "    elif all(isinstance(c, str) for c in btc_data.columns):\n",
    "        # print(\"Detected single-level string columns. Capitalizing...\") # Removed\n",
    "        btc_data.columns = [col.capitalize() for col in btc_data.columns]\n",
    "        adj_close_variations = ['Adj_close', 'Adj close']\n",
    "        adj_close_col_found = next((var for var in adj_close_variations if var in btc_data.columns), None)\n",
    "        if adj_close_col_found:\n",
    "            if 'Close' not in btc_data.columns: btc_data.rename(columns={adj_close_col_found: 'Close'}, inplace=True)\n",
    "            else: btc_data.drop(columns=[adj_close_col_found], inplace=True, errors='ignore')\n",
    "    else: print(\"Warning: Unexpected column format detected.\") # Kept Warning\n",
    "    # print(f\"Processed columns: {btc_data.columns.tolist()}\") # Removed\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during data download or initial column processing: {e}\") # Kept Error\n",
    "    raise SystemExit(f\"Stopping due to data error: {e}\")\n",
    "\n",
    "\n",
    "# === Step 1.5: Fetch Google Trends Data ===\n",
    "# print(\"\\nFetching Google Trends data for 'Bitcoin'...\") # Removed\n",
    "try:\n",
    "    pytrends = TrendReq(hl='en-US', tz=360) # Initialize\n",
    "    kw_list = [\"Bitcoin\"] # Keyword to track\n",
    "\n",
    "    # Define timeframe matching the price data\n",
    "    timeframe = f'{start_date_str} {end_date_str}'\n",
    "\n",
    "    pytrends.build_payload(kw_list, cat=0, timeframe=timeframe, geo='', gprop='')\n",
    "    trends_df = pytrends.interest_over_time()\n",
    "\n",
    "    if trends_df.empty:\n",
    "        print(\"Warning: Google Trends returned no data for the specified timeframe.\") # Kept Warning\n",
    "    else:\n",
    "        # Google Trends might return weekly data for > ~9 months. Resample to daily.\n",
    "        if pd.infer_freq(trends_df.index) == 'W-SUN' or pd.infer_freq(trends_df.index) is None: # Check if weekly or unknown freq\n",
    "             # print(\"Google Trends data appears weekly, resampling to daily and forward-filling...\") # Removed\n",
    "             # Keep only the keyword column, drop 'isPartial'\n",
    "             trends_df = trends_df[[kw_list[0]]]\n",
    "             # Resample to daily frequency, forward fill missing days\n",
    "             trends_df = trends_df.resample('D').ffill()\n",
    "        elif 'isPartial' in trends_df.columns:\n",
    "             trends_df = trends_df.drop(columns=['isPartial']) # Drop 'isPartial' if daily\n",
    "\n",
    "        # Rename column for clarity\n",
    "        trends_df.rename(columns={\"Bitcoin\": \"Bitcoin_Trend\"}, inplace=True)\n",
    "        # print(\"Google Trends data fetched and processed.\") # Removed\n",
    "        # print(trends_df.head())\n",
    "        # print(trends_df.tail())\n",
    "\n",
    "        # Merge with btc_data (use left merge to keep all btc_data rows)\n",
    "        btc_data = btc_data.merge(trends_df, left_index=True, right_index=True, how='left')\n",
    "        # Forward fill any potential gaps introduced by merging (e.g., weekends in trend data)\n",
    "        if 'Bitcoin_Trend' in btc_data.columns:\n",
    "             btc_data['Bitcoin_Trend'].ffill(inplace=True)\n",
    "             # Still might have NaNs at the very beginning if trend data starts later\n",
    "             # These will be handled by the main dropna later\n",
    "             # print(\"Merged Google Trends data into main DataFrame.\") # Removed\n",
    "        else:\n",
    "             print(\"Warning: 'Bitcoin_Trend' column not found after processing trends data.\") # Kept Warning\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching or processing Google Trends data: {e}\") # Kept Error\n",
    "    print(\"Proceeding without Google Trends feature.\") # Kept Status\n",
    "\n",
    "\n",
    "# === Step 2: Calculate Financial Features ===\n",
    "# print(\"\\nCalculating financial features...\") # Removed\n",
    "required_cols_for_ta = ['Close', 'High', 'Low', 'Volume']\n",
    "if not all(col in btc_data.columns for col in required_cols_for_ta):\n",
    "     missing = [col for col in required_cols_for_ta if col not in btc_data.columns]\n",
    "     raise KeyError(f\"Required columns {missing} not found.\")\n",
    "try:\n",
    "    btc_data['Daily_Return'] = btc_data['Close'].pct_change() * 100\n",
    "    btc_data['High_Low_Range'] = btc_data['High'] - btc_data['Low']\n",
    "    btc_data.ta.rsi(close='Close', length=14, append=True)\n",
    "    btc_data.ta.macd(close='Close', fast=12, slow=26, signal=9, append=True)\n",
    "    btc_data.ta.bbands(close='Close', length=20, std=2, append=True)\n",
    "    btc_data.ta.stoch(high='High', low='Low', close='Close', k=14, d=3, smooth_k=3, append=True)\n",
    "    btc_data.ta.obv(close='Close', volume='Volume', append=True)\n",
    "    btc_data.ta.adx(high='High', low='Low', close='Close', length=14, append=True)\n",
    "    btc_data.ta.ema(close='Close', length=5, append=True)\n",
    "    btc_data.ta.ema(close='Close', length=20, append=True)\n",
    "    btc_data.ta.atr(high='High', low='Low', close='Close', length=14, append=True)\n",
    "    log_return = np.log(btc_data['Close'] / btc_data['Close'].shift(1))\n",
    "    btc_data['Hist_Vol_30'] = log_return.rolling(window=30).std() * np.sqrt(365) * 100\n",
    "    rsi_col = next((col for col in btc_data.columns if 'RSI_14' in col), None)\n",
    "    if rsi_col is None: raise KeyError(\"RSI column not found.\")\n",
    "    btc_data['RSI_gradient'] = (btc_data[rsi_col] - btc_data[rsi_col].shift(3)) / 3\n",
    "except Exception as e: print(f\"Error during feature calculation: {e}\"); raise e # Kept Error\n",
    "\n",
    "\n",
    "# === Step 3: Create Target Variable (1-Day) & Drop NaNs ===  <--- MODIFIED COMMENT\n",
    "# print(\"\\nCreating target variable (1-day forecast) and dropping NaNs...\") # Modified comment\n",
    "if 'Close' not in btc_data.columns: raise ValueError(\"'Close' column missing.\")\n",
    "\n",
    "btc_data_1d = btc_data.copy() # <--- MODIFIED DATAFRAME NAME\n",
    "btc_data_1d['Close_Next_1'] = btc_data_1d['Close'].shift(-1) # <-- MODIFIED COLUMN NAME and Ensure shift is -1 for 1-day target\n",
    "btc_data_1d['Target'] = (btc_data_1d['Close_Next_1'] > btc_data_1d['Close']).astype(int) # <-- MODIFIED TO USE Close_Next_1\n",
    "\n",
    "initial_rows = len(btc_data_1d) # <--- MODIFIED DATAFRAME NAME\n",
    "btc_data_1d.dropna(inplace=True) # Remove NaN <--- MODIFIED DATAFRAME NAME\n",
    "# print(f\"Dropped {initial_rows - len(btc_data_1d)} total rows containing NaNs.\") # Removed\n",
    "if btc_data_1d.empty: raise ValueError(\"DataFrame empty after dropping NaNs.\") # <--- MODIFIED DATAFRAME NAME\n",
    "\n",
    "\n",
    "# === Step 4: Define Features & Split Data ===\n",
    "# print(\"\\nDefining feature set (Google Trend Included) and splitting data...\") # Removed\n",
    "if 'Target' not in btc_data_1d.columns: raise ValueError(\"Target column missing.\") # <--- MODIFIED DATAFRAME NAME\n",
    "y = btc_data_1d['Target'] # <--- MODIFIED DATAFRAME NAME\n",
    "\n",
    "all_potential_features = [\n",
    "    'Daily_Return', 'High_Low_Range', 'RSI_14', 'MACD_12_26_9', 'MACDh_12_26_9',\n",
    "    'MACDs_12_26_9', 'EMA_5', 'EMA_20', 'BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0',\n",
    "    'BBB_20_2.0', 'BBP_20_2.0', 'STOCHk_14_3_3', 'STOCHd_14_3_3', 'OBV',\n",
    "    'ADX_14', 'DMP_14', 'DMN_14', 'Hist_Vol_30', 'RSI_gradient',\n",
    "    'ATRr_14', # Placeholder\n",
    "    'ROC_10', 'WILLR_14', 'CMF_20', # Previously added features\n",
    "    'Bitcoin_Trend'\n",
    "]\n",
    "\n",
    "# Ensure correct RSI column name is used if pandas_ta changed it (e.g., RSI_14_x)\n",
    "rsi_col_actual = next((col for col in btc_data_1d.columns if 'RSI' in col.upper() and '14' in col), 'RSI_14') # <--- MODIFIED DATAFRAME NAME\n",
    "if 'RSI_14' in all_potential_features and rsi_col_actual != 'RSI_14':\n",
    "    all_potential_features[all_potential_features.index('RSI_14')] = rsi_col_actual\n",
    "\n",
    "# Ensure correct ATR column name is used (e.g., ATRr_14)\n",
    "atr_col_actual = next((col for col in btc_data_1d.columns if 'ATR' in col.upper() and '14' in col), 'ATRr_14') # <--- MODIFIED DATAFRAME NAME\n",
    "if 'ATRr_14' in all_potential_features and atr_col_actual != 'ATRr_14':\n",
    "    all_potential_features[all_potential_features.index('ATRr_14')] = atr_col_actual\n",
    "\n",
    "\n",
    "X_cols = [col for col in all_potential_features if col in btc_data_1d.columns] # <--- MODIFIED DATAFRAME NAME\n",
    "\n",
    "\n",
    "if 'Bitcoin_Trend' not in btc_data_1d.columns: # <--- MODIFIED DATAFRAME NAME\n",
    "    print(\"Warning: 'Bitcoin_Trend' feature not found, excluding it.\") # Kept Warning\n",
    "    if 'Bitcoin_Trend' in X_cols: X_cols.remove('Bitcoin_Trend')\n",
    "elif 'Bitcoin_Trend' not in X_cols: # Add if found but not in list for some reason\n",
    "    X_cols.append('Bitcoin_Trend')\n",
    "\n",
    "\n",
    "# print(f\"Features used for X: {X_cols}\") # Removed\n",
    "# print(f\"Number of features: {len(X_cols)}\") # Removed\n",
    "X = btc_data_1d[X_cols] # <--- MODIFIED DATAFRAME NAME\n",
    "\n",
    "# Split data (70/15/15)\n",
    "train_size_pct = 0.70; val_size_pct = 0.15; n_total = len(X)\n",
    "n_train = int(n_total * train_size_pct); n_val = int(n_total * val_size_pct); n_test = n_total - n_train - n_val\n",
    "if n_train <= 0 or n_val <= 0 or n_test <= 0: raise ValueError(f\"Invalid split sizes for {n_total} samples.\")\n",
    "X_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\n",
    "X_val, y_val = X.iloc[n_train:n_train + n_val], y.iloc[n_train:n_train + n_val]\n",
    "X_test, y_test = X.iloc[n_train + n_val:], y.iloc[n_train + n_val:]\n",
    "# print(f\"Data split into Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\") # Removed\n",
    "\n",
    "# === Step 5: Apply Scaling ===\n",
    "# (No changes needed in Step 5, as it uses X_train, X_val, X_test which are already defined based on the modified X)\n",
    "# print(\"\\nApplying StandardScaler...\") # Removed\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "# print(\"Scaling complete.\") # Removed\n",
    "\n",
    "# === Step 6: Train Tuned XGBoost Model ===\n",
    "# (No changes needed in Step 6, model trains on the prepared scaled data)\n",
    "# print(\"\\nTraining Tuned XGBoost model (Google Trend Included)...\") # Removed\n",
    "best_params_xgb = {'subsample': 0.6, 'reg_lambda': 1, 'reg_alpha': 0.01, 'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.1, 'gamma': 0, 'colsample_bytree': 0.8}\n",
    "xgb_specific_params = {k: v for k, v in best_params_xgb.items() if k in xgb.XGBClassifier().get_params()}\n",
    "# print(f\"Using Tuned XGBoost Params: {xgb_specific_params}\") # Removed\n",
    "model_with_trend = xgb.XGBClassifier( objective='binary:logistic', use_label_encoder=False, **xgb_specific_params, random_state=42)\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    model_with_trend.fit(X_train_scaled, y_train, verbose=False)\n",
    "    # print(\"Training complete.\") # Removed\n",
    "\n",
    "    # === Step 7: Evaluate on Test Set ===\n",
    "    # (No changes needed in Step 7, evaluation uses the trained model and test sets)\n",
    "    # print(\"Making predictions on Test Set...\") # Removed\n",
    "    y_pred_test_trend = model_with_trend.predict(X_test_scaled)\n",
    "    y_pred_proba_test_trend = model_with_trend.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # --- Keep Final Output ---\n",
    "    # Modify the title print statement if desired for clarity\n",
    "    print(\"\\n--- FINAL Model Evaluation on TEST SET (1-Day Prediction, Google Trend Included) ---\") # <-- MODIFIED TITLE\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_test_trend):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred_test_trend, zero_division=0):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred_test_trend, zero_division=0):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred_test_trend, zero_division=0):.4f}\")\n",
    "    try:\n",
    "        if len(np.unique(y_test)) > 1:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba_test_trend)\n",
    "            print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "        else: print(\"ROC-AUC Score: Not defined (only one class in test set)\") # Modified for clarity\n",
    "    except ValueError as e: print(f\"Could not calculate ROC-AUC: {e}\") # Kept Error\n",
    "\n",
    "    # --- Feature Importance ---\n",
    "    # Modify the title print statement if desired for clarity\n",
    "    print(\"\\nFeature Importances (1-Day Prediction Model, Google Trend Included):\") # <-- MODIFIED TITLE\n",
    "    importances_trend = model_with_trend.feature_importances_\n",
    "    feature_names_trend = X_train_scaled.columns\n",
    "    final_importances_trend = pd.Series(importances_trend, index=feature_names_trend).sort_values(ascending=False)\n",
    "    print(final_importances_trend) # Keep feature importance list\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 9))\n",
    "    try:\n",
    "        sns.barplot(x=final_importances_trend, y=final_importances_trend.index)\n",
    "        # Modify plot title if desired\n",
    "        plt.title('Feature Importances (1-Day Prediction, Google Trend Included)') # <-- MODIFIED TITLE\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except NameError: print(\"\\n(Seaborn not imported, skipping plot)\") # Kept Warning\n",
    "\n",
    "# Keep Error Handling Blocks\n",
    "except NameError as e: print(f\"Error: Required variables not found. Details: {e}\")\n",
    "except KeyError as e: print(f\"Error: Column key error. Details: {e}\")\n",
    "except ValueError as e: print(f\"Error: Value error (check data/split). Details: {e}\") # Added more specific error type\n",
    "except Exception as e: print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110455a-65b9-465c-a057-0562b909b1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
